{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4f9d93c5",
      "metadata": {
        "id": "4f9d93c5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "!pip install rouge-score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e14d038",
      "metadata": {
        "id": "1e14d038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63fe219-dae1-424a-cb63-5fb7b0e87f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec  6 16:34:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   67C    P8              19W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, MT5ForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb\n",
        "\n",
        "!nvidia-smi\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "233b44d2",
      "metadata": {
        "id": "233b44d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595cf7af-4b2a-4d94-f8ca-b78a222d0d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marsitha-lbf315\u001b[0m (\u001b[33marsitha-lbf315-george-mason-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Login to wandb to log the model run and all the parameters\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "41ff0d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "41ff0d38",
        "outputId": "a400f758-bdb6-4c1d-c344-817b41b27621"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241206_163421-hdsfbpcj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arsitha-lbf315-george-mason-university/transformers_tutorials_summarization/runs/hdsfbpcj' target=\"_blank\">mild-voice-72</a></strong> to <a href='https://wandb.ai/arsitha-lbf315-george-mason-university/transformers_tutorials_summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arsitha-lbf315-george-mason-university/transformers_tutorials_summarization' target=\"_blank\">https://wandb.ai/arsitha-lbf315-george-mason-university/transformers_tutorials_summarization</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arsitha-lbf315-george-mason-university/transformers_tutorials_summarization/runs/hdsfbpcj' target=\"_blank\">https://wandb.ai/arsitha-lbf315-george-mason-university/transformers_tutorials_summarization/runs/hdsfbpcj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             summary  \\\n",
            "0  పేద కుటుంబం నుండి వచ్చిన రాజేష్, తన విద్య కోసం...   \n",
            "1  వృద్ధుడైన శాస్త్రి తన విలువైన తోటను కాపాడటం కో...   \n",
            "2  ఆకాశంలోని మబ్బులను ప్రేమించే చిన్నారి రవి జీవి...   \n",
            "3  చీన దేవగిరి కోటలో నివసించే ఒక వృద్ధ శిల్పి జీవ...   \n",
            "4  రాము అనే బాలుడు చిన్న పల్లెటూరిలో పుట్టి పెరిగ...   \n",
            "\n",
            "                                               story  \n",
            "0  summarize: రాజేష్ అనే కుర్రవాడు చిన్న పల్లెటూర...  \n",
            "1  summarize: శాస్త్రి అనే వృద్ధుడు తన వంట చెట్ల ...  \n",
            "2  summarize: వసంత పల్లెలో ఒక చిన్న పిల్లవాడు రవి...  \n",
            "3  summarize: దేవగిరి అనే ప్రాచీన కోటలో ఒక వృద్ధ ...  \n",
            "4  summarize: రాము అనే చిన్న బాలుడు తన తల్లిదండ్ర...  \n",
            "FULL Dataset: (200, 2)\n",
            "TRAIN Dataset: (160, 2)\n",
            "TEST Dataset: (40, 2)\n",
            "Starting Fine-Tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 5.697854995727539\n",
            "Completed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6, 'no_repeat_ngram_size': 2}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model at epoch 0\n",
            "Epoch: 1, Loss: 1.9551866054534912\n",
            "Epoch: 2, Loss: 1.6142200231552124\n",
            "Epoch: 3, Loss: 1.4035757780075073\n",
            "Epoch: 4, Loss: 1.3729761838912964\n",
            "Epoch: 5, Loss: 0.9112350344657898\n",
            "Completed 0\n",
            "Saved best model at epoch 5\n",
            "Epoch: 6, Loss: 0.796274721622467\n",
            "Epoch: 7, Loss: 0.6849696040153503\n",
            "Epoch: 8, Loss: 0.5307756662368774\n",
            "Epoch: 9, Loss: 0.588497519493103\n",
            "Epoch: 10, Loss: 0.43642503023147583\n",
            "Completed 0\n",
            "Saved best model at epoch 10\n",
            "Epoch: 11, Loss: 0.34087103605270386\n",
            "Epoch: 12, Loss: 0.28361260890960693\n",
            "Epoch: 13, Loss: 0.3387318551540375\n",
            "Epoch: 14, Loss: 0.3234672546386719\n",
            "Epoch: 15, Loss: 0.22598335146903992\n",
            "Completed 0\n",
            "Saved best model at epoch 15\n",
            "Epoch: 16, Loss: 0.246625617146492\n",
            "Epoch: 17, Loss: 0.15996159613132477\n",
            "Epoch: 18, Loss: 0.1998007446527481\n",
            "Epoch: 19, Loss: 0.1707136332988739\n",
            "Epoch: 20, Loss: 0.1715845912694931\n",
            "Completed 0\n",
            "Saved best model at epoch 20\n",
            "Epoch: 21, Loss: 0.10438496619462967\n",
            "Epoch: 22, Loss: 0.14618141949176788\n",
            "Epoch: 23, Loss: 0.1172698587179184\n",
            "Epoch: 24, Loss: 0.12506215274333954\n",
            "Epoch: 25, Loss: 0.09866424649953842\n",
            "Completed 0\n",
            "Saved best model at epoch 25\n",
            "Epoch: 26, Loss: 0.11994949728250504\n",
            "Epoch: 27, Loss: 0.0783814936876297\n",
            "Epoch: 28, Loss: 0.057180941104888916\n",
            "Epoch: 29, Loss: 0.06324921548366547\n",
            "Generating final summaries\n",
            "Completed 0\n",
            "Output files generated\n",
            "Final model saved to: ./fine_tuned_mt5_telugu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import T5Tokenizer, MT5ForConditionalGeneration\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from rouge_score import rouge_scorer\n",
        "import wandb\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for handling Telugu text data.\n",
        "    Prepares data for both original stories and generated summaries.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.summary = self.data.summary\n",
        "        self.story = self.data.story\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.summary)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        story = str(self.story[index])\n",
        "        story = ' '.join(story.split())\n",
        "        summary = str(self.summary[index])\n",
        "        summary = ' '.join(summary.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [story],\n",
        "            max_length=self.source_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [summary],\n",
        "            max_length=self.summ_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    \"\"\"\n",
        "    Training function that performs training on the give dataset. It also includes loss calculation and optimization.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        labels = y[:, 1:].clone().detach()\n",
        "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        ids = data['source_ids'].to(device, dtype=torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _ % 500 == 0:\n",
        "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    \"\"\"\n",
        "    Validation function to generate summaries and calculate metrics.\n",
        "    Returns predictions and actual summaries for comparison.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype=torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype=torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=400,\n",
        "                min_length=200,\n",
        "                do_sample=True,\n",
        "                num_beams=8,\n",
        "                no_repeat_ngram_size=4,\n",
        "                temperature=0.7,\n",
        "                length_penalty=2.0,\n",
        "                early_stopping=False,\n",
        "                top_k=50,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.8,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "\n",
        "            if _ % 100 == 0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "    # Configuration\n",
        "    config = wandb.config\n",
        "    config.TRAIN_BATCH_SIZE = 4\n",
        "    config.VALID_BATCH_SIZE = 4\n",
        "    config.TRAIN_EPOCHS = 30\n",
        "    config.VAL_EPOCHS = 1\n",
        "    config.LEARNING_RATE = 2e-4\n",
        "    config.SEED = 42\n",
        "    config.MAX_LEN = 512\n",
        "    config.SUMMARY_LEN = 150\n",
        "\n",
        "    # Set random seeds\n",
        "    torch.manual_seed(config.SEED)\n",
        "    np.random.seed(config.SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\", legacy=True)\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    df = pd.read_excel('telugu_stories.xlsx')\n",
        "    df = df[['summary', 'story']]\n",
        "    df.story = 'summarize: ' + df.story\n",
        "    print(df.head())\n",
        "\n",
        "    # Split data\n",
        "    train_size = 0.8\n",
        "    train_dataset = df.sample(frac=train_size, random_state=config.SEED)\n",
        "    val_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(f\"FULL Dataset: {df.shape}\")\n",
        "    print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    print(f\"TEST Dataset: {val_dataset.shape}\")\n",
        "\n",
        "    # Create datasets\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 2\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': config.VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 2\n",
        "    }\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.LEARNING_RATE)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "    # Log metrics with wandb\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    # Training loop\n",
        "    print('Starting Fine-Tuning')\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(config.TRAIN_EPOCHS):\n",
        "        avg_loss = train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        # # Validating every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "\n",
        "            # Save the model if validation loss is improved\n",
        "            if avg_loss < best_valid_loss:\n",
        "                best_valid_loss = avg_loss\n",
        "                model.save_pretrained('./best_model')\n",
        "                tokenizer.save_pretrained('./best_model')\n",
        "                print(f\"Saved best model at epoch {epoch}\")\n",
        "\n",
        "    # Final validation and save predictions\n",
        "    print('Generating final summaries')\n",
        "    predictions, actuals = validate(0, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text': predictions, 'Actual Text': actuals})\n",
        "    final_df.to_csv('predictions.csv')\n",
        "    print('Output files generated')\n",
        "\n",
        "    # Save the final model\n",
        "    output_dir = \"./fine_tuned_mt5_telugu\"\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Final model saved to: {output_dir}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "64ea492b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ea492b",
        "outputId": "9e5370a5-73d2-41db-ec7f-4b689f2842e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Telugu Story Summarizer!\n",
            "Enter your story in Telugu and get a summary.\n",
            "Type 'quit' to exit the program.\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter your Telugu story (or 'quit' to exit):\n",
            "అనగనగా, ఒక పెద్ద అడవిలో రామచిలుక అనే తెలివైన పక్షి నివసించేది. ఇది ఎల్లప్పుడూ ఇతర పక్షులకు సహాయం చేస్తూ, స్నేహపూర్వకంగా ఉండేది. అడవిలోని ప్రతి జంతువుకు ఇది ఎంతో ఇష్టమైన పక్షి. రామచిలుక తన తెలివితేటలతో సమస్యలను పరిష్కరించడంలో అందరికీ ఆదర్శంగా నిలిచేది. ఒకసారి, ఆ అడవిని ఒక పెద్ద ఎడారి తాకింది. ఎడారి ప్రభావం అడవిని నాశనం చేయడం ప్రారంభించింది. చెట్లు వాడిపోవడం, నీటి వనరులు ఎండిపోవడం, జంతువుల ఆహారం అందకపోవడం వంటి సమస్యలు కలిగాయి. అడవిలోని జంతువులన్నీ ఈ పరిస్థితిని ఎదుర్కోలేక అల్లాడిపోతున్నాయి. రామచిలుక ఈ పరిస్థితిని గమనించి, \"నేను ఒక మార్గం కనుగొని, ఈ సమస్యను పరిష్కరిస్తాను,\" అని చెప్పింది. అది తన ప్రయాణాన్ని ప్రారంభించింది. దూరంగా ఉన్న పచ్చని ప్రాంతాలను కనుగొనడానికి అటూ ఇటూ ఎగిరింది. పక్షి అయినందున, అది ఆకాశం నుండి దూర ప్రాంతాలను పరిశీలించగలిగింది. కొన్ని రోజుల ప్రయాణం తర్వాత, రామచిలుక ఒక పెద్ద సరస్సును కనుగొంది. ఆ సరస్సు పక్కన పచ్చని చెట్లు, అందమైన పూలు, సమృద్ధిగా నీరు కనిపించాయి. రామచిలుక వెంటనే తన అడవికి తిరిగి వచ్చి, అక్కడి జంతువులకు ఈ శుభవార్త చెప్పింది. \"అదే అడవి మొత్తం జీవితం సాధించడానికి సరైన ప్రదేశం,\" అని చెప్పింది. కానీ, ఎడారిని దాటడం చాలా కష్టం. జంతువులన్నీ ఆ దూర ప్రయాణాన్ని చేయగలమా అని అనుమానంతో ఉన్నాయి. రామచిలుక వారికి ధైర్యం చెప్పింది. \"నేను ముందుండి మార్గాన్ని చూపిస్తాను. మీరు కలసికట్టుగా నడిస్తే, ఈ ప్రయాణం సులభమవుతుంది,\" అని చెప్పింది. పక్షి సూచనలను అనుసరించి, అన్ని జంతువులు ప్రయాణాన్ని ప్రారంభించాయి. దారిలో అనేక అడ్డంకులను ఎదుర్కొన్నప్పటికీ, రామచిలుక తన తెలివితేటలతో ప్రతిదాన్ని పరిష్కరించింది. ఎడారిలో నీటి లేకుండా కొన్ని జంతువులు కడలిపోయాయి. అప్పుడు రామచిలుక దగ్గరలో ఉన్న చిన్న ఓయాను కనుగొని, జంతువులకు నీరు అందించింది. ఒకసారి పగటి వేడి ఎక్కువైనప్పుడు, పక్షి పెద్ద చెట్ల చీకటి ప్రదేశాలను కనుగొని అందరికీ విశ్రాంతి కల్పించింది. ఈ ప్రయాణంలో రామచిలుక తన ధైర్యం, పట్టుదలతో జంతువులకు ఒక నాయకురాలిగా నిలిచింది. కొన్ని వారాల ప్రయాణం తర్వాత, జంతువులన్నీ సరస్సు దగ్గరికి చేరాయి. అందరూ కొత్త ప్రదేశాన్ని చూసి ఆశ్చర్యపోయారు. అక్కడి పచ్చదనాన్ని, సమృద్ధి కలిగిన వనరులను చూసి, వారి కళ్లలో ఆనందభాష్పాలు కనిపించాయి. రామచిలుకకు జంతువులన్నీ కృతజ్ఞతలు తెలిపారు. \"నీ ధైర్యం, తెలివి, మరియు నాయకత్వం లేకపోతే, మేము ఈ ప్రదేశానికి చేరుకోలేము,\" అని అన్నాయి. ఆ కొత్త అడవి అన్ని జంతువులకు ఒక కొత్త జీవితాన్ని ఇచ్చింది. ఆ రోజు నుండి, రామచిలుక కొత్త అడవిలోని పక్షులకు నాయకురాలిగా మారింది. అందరూ కలసికట్టుగా జీవిస్తూ, ఆ ప్రదేశాన్ని మరింత అందంగా మార్చారు. ఈ కథ రామచిలుక ధైర్యానికి, తెలివికి, మరియు సహాయస్పూర్తికి చక్కని ఉదాహరణగా నిలిచింది.\n",
            "\n",
            "Generating Abstarctive summary...\n",
            "\n",
            "Generated Abstractie Summary:\n",
            "--------------------\n",
            "రామచిలుక తన తెలివితేటలతో అడవిలోని జంతువులను రక్షించింది. ఎడారి ప్రభావం అడవిని నాశనం చేయడం, చెట్లు వాడిపోవడం, జంతువుల ఆహారం అందకపోవడం వంటి సమస్యలను పరిష్కరించడంలో చూపిన తెలివితో, అది తన ప్రత్యేకమైన ప్రయాణాన్ని ప్రారంభించింది. సరస్సు ద్వారా పచ్చని చెట్లు, పూలు, అందమైన పూలను కనుగొంది. ఈ ప్రయాణం అడవికి తిరిగి వచ్చి, అక్కడి జంతువులకు శుభవార్త చెప్పింది. జంతువులందరూ ఆ ప్రయాణాన్ని కలసి, కలసికట్టుగా నడవడం ద్వారా ప్రతిదాన్ని పరిష్కరించింది. ఈ ప్రయాణంలో జంతువులన్నీ అడ్డంకులను ఎదుర్కొన్నా, ఎడారిని దాటడం ద్వారా జంతువుల జీవితం మెరుగుపడటానికి మార్గదర్శకత్వం వహించి, ప్రతి జంతువును రక్షించగలిగేలా మారాయి.\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter your Telugu story (or 'quit' to exit):\n",
            "అనగనగా, ఒక తక్కువ లోతైన సరస్సులో అనేక చేపలు నివసించేవి. అందులో గంగమ్మ అనే ఒక చేప అందరికీ రాణి. గంగమ్మ అందంగా ఉండడమే కాకుండా, చాలా తెలివైనది. అది తన గుంపులోని అన్ని చేపలను ప్రేమతో చూసేది. ప్రతి రోజు ఆ సరస్సులోని పచ్చని మొక్కల మధ్య ఆడుకుంటూ, సరదాగా గడుపుతూ ఉండేవి. ఒక రోజు, ఆ సరస్సులో మత్స్యకారులు వచ్చారు. వారు పెద్దవలతో చేపలను పట్టుకోవడానికి సిద్ధమయ్యారు. మత్స్యకారుల కదలికలను గంగమ్మ చూసింది. అది వెంటనే ఇతర చేపలను హెచ్చరించింది. \"వాళ్లు మనలను పట్టుకోడానికి వచ్చారు. మనం ఇప్పుడు ఏం చేయాలో ఆలోచించాలి,\" అని చెప్పింది. కొన్ని చేపలు భయపడి పారిపోయాయి, మరికొన్ని ఏమి చేయాలో అర్థం కాకుండా ఉండిపోయాయి. గంగమ్మ తన తెలివితో ఒక వ్యూహం ఆలోచించింది. \"మనం పచ్చని మొక్కల మధ్య దాక్కోవాలి. అలాగే, పాత బండరాళ్ల కింద తలదాచుకోవాలి. ఎవరూ వలకు చిక్కకూడదు,\" అని సూచించింది. అందరూ గంగమ్మ చెప్పిన మాటను పాటించాయి. కొన్ని చేపలు మొక్కల మధ్య దాక్కున్నాయి, మరికొన్ని సరస్సు లోతైన భాగంలోకి వెళ్లాయి. మత్స్యకారులు వల విసిరినా, పెద్దగా చేపలు చిక్కలేదు. వలలో ఉన్న కొద్దిమందిని గంగమ్మ గుంపు కలిసి కలిసి బయటికి తీసుకొచ్చింది. ఇది మత్స్యకారుల దృష్టిలో పడలేదు. మత్స్యకారులు అవస్థపడి, ఆ సరస్సును వదిలి వెళ్లిపోయారు. గంగమ్మ తన తెలివి, ధైర్యంతో తన గుంపును కాపాడింది. తరువాత, ఆ సరస్సులోని చేపలన్నీ గంగమ్మను మెచ్చుకున్నాయి. అవి దానికి రాణిగా మరింత గౌరవం చూపించాయి. గంగమ్మ తన గుంపు కోసం ఎల్లప్పుడూ అప్రమత్తంగా ఉండేది. అదే సరస్సులో, పక్క ప్రాంతాల నుండి వచ్చిన చేపలు కూడా గంగమ్మ వద్దకు వచ్చి సహాయం పొందాయి. తర్వాత ఆ సరస్సు అన్ని చేపలకూ సురక్షిత ప్రదేశంగా మారింది. గంగమ్మ తన తెలివితేటలతో మాత్రమే కాకుండా, తన ప్రేమ, జాగ్రత్తలతో కూడా ఆ చేపల గుండెల్లో శాశ్వతంగా నిలిచింది. గంగమ్మ కథ కొన్ని తరాలపాటు చెరగని గుర్తుగా నిలిచింది.\n",
            "\n",
            "Generating Abstarctive summary...\n",
            "\n",
            "Generated Abstractie Summary:\n",
            "--------------------\n",
            "ఆ సరస్సులో గంగమ్మ అనే తెలివైన చేప రాణిగా జీవించేది. ఇది తన గుంపు చేపలందరినీ ప్రేమగా, జాగ్రత్తగా చూసుకునేది. ఒక రోజు, మత్స్యకారులు పెద్ద వలతో సరస్సులో చేపలను పట్టుకోవడానికి వచ్చారు. గంగమ్మ మత్ద్కారుల కదలికలను గమనించి, తన చేపల గుంపును రక్షించడానికి ఒక వ్యూహాన్ని ఆలోచించింది. ఇతర చేపలను పచ్చని మొక్కల మధ్య దాక్కోవాలని, సరస్సు లోతైన భాగాల్లో తలదాచుకోవాలని చెప్పింది. చేపలన్నీ గంగమ్మ సూచనను పాటించాయి. వలలో చిక్కుకున్న కొద్ది మంది చేపలను గంగమ్మ గుంపును కాపాడాయి. ఈ సంఘటన తర్వాత, సరస్సు అన్ని చేపలకూ సురక్షిత ప్రదేశంగా మారింది. చేపలంతా గంగమ్మను మెచ్చుకున్నాయి.\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter your Telugu story (or 'quit' to exit):\n",
            "quit\n",
            "Thank you for using Telugu Story Summarizer!\n"
          ]
        }
      ],
      "source": [
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "def generate_summary(text, model_path=\"./fine_tuned_mt5_telugu\"):\n",
        "    \"\"\"\n",
        "    Generate summary for a given Telugu text using the trained model.\n",
        "    that takes Args as text: Input Telugu text, model_path: Path to the saved model\n",
        "    and returns generated summary in Telugu.\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer(f\"summarize: {text}\", return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=400,\n",
        "            min_length=200,\n",
        "            num_beams=8,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            length_penalty=2.0,\n",
        "            no_repeat_ngram_size=4,\n",
        "            early_stopping=False,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.8,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to Telugu Story Summarizer!\")\n",
        "    print(\"Enter your story in Telugu and get a summary.\")\n",
        "    print(\"Type 'quit' to exit the program.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"\\nEnter your Telugu story (or 'quit' to exit):\")\n",
        "            story = input()\n",
        "\n",
        "            if story.lower() == 'quit':\n",
        "                print(\"Thank you for using Telugu Story Summarizer!\")\n",
        "                break\n",
        "\n",
        "            if not story.strip():\n",
        "                print(\"Please enter a valid story!\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\nGenerating Abstarctive summary...\")\n",
        "            summary = generate_summary(story)\n",
        "            print(\"\\nGenerated Abstractie Summary:\")\n",
        "            print(\"-\" * 20)\n",
        "            print(summary)\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError occurred: {str(e)}\")\n",
        "            print(\"Please try again with a different story.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5f2ac0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f5f2ac0",
        "outputId": "92130bd6-40f6-4662-db90-ec274af5a69b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "[{'rouge-1': {'r': 0.49056603773584906, 'p': 0.8666666666666667, 'f': 0.6265060194803311}, 'rouge-2': {'r': 0.37681159420289856, 'p': 0.7536231884057971, 'f': 0.5024154544927536}, 'rouge-l': {'r': 0.49056603773584906, 'p': 0.8666666666666667, 'f': 0.6265060194803311}}]\n"
          ]
        }
      ],
      "source": [
        "#moderate-length story\n",
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "Generated_summary = \"ఆ సరస్సులో గంగమ్మ అనే తెలివైన చేప రాణిగా జీవించేది. ఇది తన గుంపు చేపలందరినీ ప్రేమగా, జాగ్రత్తగా చూసుకునేది. ఒక రోజు, మత్స్యకారులు పెద్ద వలతో సరస్సులో చేపలను పట్టుకోవడానికి వచ్చారు. గంగమ్మ మత్ద్కారుల కదలికలను గమనించి, తన చేపల గుంపును రక్షించడానికి ఒక వ్యూహాన్ని ఆలోచించింది. ఇతర చేపలను పచ్చని మొక్కల మధ్య దాక్కోవాలని, సరస్సు లోతైన భాగాల్లో తలదాచుకోవాలని చెప్పింది. చేపలన్నీ గంగమ్మ సూచనను పాటించాయి. వలలో చిక్కుకున్న కొద్ది మంది చేపలను గంగమ్మ గుంపును కాపాడాయి. ఈ సంఘటన తర్వాత, సరస్సు అన్ని చేపలకూ సురక్షిత ప్రదేశంగా మారింది. చేపలంతా గంగమ్మను మెచ్చుకున్నాయి.\"\n",
        "Actual_summary = \"తక్కువ లోతైన సరస్సులో గంగమ్మ అనే తెలివైన చేప రాణిగా జీవించేది. ఇది తన గుంపు చేపలందరినీ ప్రేమగా, జాగ్రత్తగా చూసుకునేది. ఒక రోజు, మత్స్యకారులు పెద్ద వలతో సరస్సులో చేపలను పట్టుకోవడానికి వచ్చారు. గంగమ్మ మత్స్యకారుల కదలికలను గమనించి, తన చేపల గుంపును రక్షించడానికి ఒక వ్యూహాన్ని ఆలోచించింది.ఇతర చేపలను పచ్చని మొక్కల మధ్య దాక్కోవాలని, సరస్సు లోతైన భాగాల్లో తలదాచుకోవాలని చెప్పింది. చేపలన్నీ గంగమ్మ సూచనను పాటించాయి. వలలో చిక్కిన కొద్దిమందిని కూడా గంగమ్మ తన గుంపుతో కలిసి బయటికి తీసుకొచ్చింది. మత్స్యకారులు వల విసిరినా పెద్దగా చేపలు చిక్కకపోవడంతో అవస్థపడి సరస్సును వదిలి వెళ్లిపోయారు.తరువాత, సరస్సులోని చేపలన్నీ గంగమ్మను తమ రక్షకురాలిగా భావించి మరింత గౌరవించాయి. ఇతర ప్రాంతాల నుండి వచ్చిన చేపలు కూడా గంగమ్మ వద్ద సహాయం పొందుతూ, తన తెలివితేటలను మెచ్చుకున్నాయి. గంగమ్మ తన గుంపుకు ప్రేమతో మాత్రమే కాకుండా, జాగ్రత్తలతో కూడా రాణిగా నిలిచింది.ఆ సరస్సు అన్ని చేపలకూ సురక్షిత ప్రదేశంగా మారింది. గంగమ్మ కథ చేపల గుంపు మధ్య మాత్రమే కాకుండా, ఇతర ప్రాంతాల మత్స్యకుల కథల్లోనూ ప్రసిద్ధిగా నిలిచింది. గంగమ్మ తన తెలివి, ధైర్యంతో, మరియు నిరంతరం ప్రేమతో సముదాయానికి ఒక స్ఫూర్తిదాయక నేతగా నిలిచింది. ఈ కథ పాత తరాలపాటు చేపల గుంపుల్లో చెరగని గుర్తుగా నిలిచింది.\"\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(Generated_summary ,Actual_summary)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q65-NxxjcA02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q65-NxxjcA02",
        "outputId": "7d941a8e-de5e-4cb0-d233-690ad8ba7d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "[{'rouge-1': {'r': 0.2638888888888889, 'p': 0.30158730158730157, 'f': 0.28148147650370375}, 'rouge-2': {'r': 0.05, 'p': 0.06329113924050633, 'f': 0.05586591685652802}, 'rouge-l': {'r': 0.2638888888888889, 'p': 0.30158730158730157, 'f': 0.28148147650370375}}]\n"
          ]
        }
      ],
      "source": [
        "#short story\n",
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "Generated_summary = \"బబుజ్జి ఒక తోటలో పడి ఉన్న పూవులను చూసి ఆనందంగా గడిపాడు. ఆ పూవులను జాగ్రత్తగా ఏరుకుని, తన అమ్మకు అందించేందుకు ఒక అందమైన పూలగుచ్చం కట్టాడు. ఆ గుచ్చం సువాసనతో కాదు, తన ముద్దు మనసుతో కూడా నిండిపోయింది. బుజ్జి ప్రతి ఉదయం తన తల్లికి పూలతో అనుభూతిని ఇవ్వడం ప్రారంభించాడు. ఆ రోజు నుండి బుజ్జి తన స్నేహితులకు కొత్త అనుభూతిని ఇచ్చాడు. పూలు పంచుకున్న తల్లి బుజ్జికి కొత్త ఆశాజ్యోతులుగా నిలిచాడు. ఈ అనుభవం తర్వాత బుజ్జి బాలుడి జీవితం మళ్ళీ పూలంతో కళకళలాడింది. పిల్లలతో ఆడుకుంటూ బుజ్జి కష్టపడితే ఏడాది చివర్లో పూలంతో ఆనందంగా జీవించడం ప్రారంభించాడు. అతను చిన్న బాలుడి నుండి పూల ఆశీర్వాదం పొందాడు.\"\n",
        "Actual_summary = \"బుజ్జి అనే చిన్న బాలుడు తన తోటలో పడి ఉన్న పూలను జాగ్రత్తగా ఏరుకుని, వాటిని ఒక అందమైన పూలగుచ్చంగా మార్చాడు. తల్లికి తన ప్రేమను వ్యక్తం చేయడంలో ఈ పూలగుచ్చం ఒక మంచి సాధనమైంది. తల్లి బుజ్జి చేసిన పనిని చూసి ఎంతో ఆనందించగా, బుజ్జి తల్లి ముఖంలో సంతోషాన్ని చూడటం తనకు పెద్ద ఆనందంగా అనిపించింది.ఈ సంఘటనతో బుజ్జి ప్రతిరోజూ తోటలోకి వెళ్లి కొత్త పూలను ఏరుకొని, తన తల్లికి అందించడాన్ని అలవాటుగా మార్చుకున్నాడు. బుజ్జి తల్లి పూల సువాసనలో కంటే, బుజ్జి మనసులో ఉన్న ప్రేమను మరింత ప్రశంసించింది. తల్లి, తన కుమారుడి ఈ చిన్న కానుకలో అనేక భావోద్వేగాలను చూసి మురిసిపోయింది.ఈ చిన్న పూలగుచ్చం తల్లికుమారుల అనుబంధంలో మరింత గాఢతను తెచ్చింది. బుజ్జి ప్రేమతో చేసిన ప్రతి పూలగుచ్చం, తల్లికి ప్రతిరోజూ కొత్త ఆనందాన్ని ఇచ్చింది. తోటలో పూసే ప్రతి పువ్వు ఈ బంధానికి జీవం పోసింది.\"\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(Generated_summary ,Actual_summary)\n",
        "print(scores)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}